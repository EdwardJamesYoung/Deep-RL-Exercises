{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cartpole environment\n",
    "\n",
    "Cartpole is a classic RL control task. A cart lies on a track and has a pole attached to it which can freely rotate. At each time step, the cart may choose to move left or right. The goal is to keep the pole upright for as long as possible. \n",
    "\n",
    "### Termination and truncations conditions\n",
    "The environment terminates if the pole angle is more than 12 degrees from vertical, or if the cart position is more than 2.4 units from the center.\n",
    "The environment is truncated after 500 time steps, if the pole is still upright.\n",
    "\n",
    "### Rewards\n",
    "The agent recieves a reward of +1 at each time step, as long as the pole is upright.\n",
    "\n",
    "### Observations\n",
    "At each time point, the RL agent recieves the following observations:\n",
    "1. Cart position\n",
    "2. Cart velocity \n",
    "3. Pole angle from vertical\n",
    "4. Pole anglular velocity \n",
    "\n",
    "### Action space\n",
    "The agent can take one of two actions at each time step:\n",
    "1. Move the cart to the left\n",
    "2. Move the cart to the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN algorithm components\n",
    "\n",
    "The DQN algorithm requires the following components, with the following roles:\n",
    "1. **Value network**: A neural network that takes in states and outputs predictions for the value of each action in that state\n",
    "2. **Target network**: A copy of the value network with lagged parameters that is used to compute the target values for the regression targets\n",
    "3. **Replay buffer**: A buffer that stores transitions (state, action, reward, next_state, terminated) that the agent has experienced. This is used to sample mini-batches of transitions to train the value network\n",
    "4. **Policies**: These use the value network to select actions in the environment. We will consider two policies:\n",
    "    - **Greedy policy**: This policy selects the action with the highest value, and is used to evaluate the value network\n",
    "    - **$\\epsilon$-greedy policy**: This policy selects a random action with probability $\\epsilon$, and the action with the highest value with probability 1-$\\epsilon$. This is used to explore the environment using the training phase. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The value and target networks\n",
    "\n",
    "We start by defining the value network architecture. This is a simple feedforward neural network (a **multi-layer perceptron**). \n",
    "The input to the network is the state of the environment, and the output is the value prediction of each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q_net class. This inherits from the nn.Module class\n",
    "\n",
    "    # Initialise the network using the size of the observation space and the number of actions\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        # Use the nn.Module's __init__ method to ensure that the parameters can be updated during training\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the network\n",
    "        \n",
    "\n",
    "\n",
    "    # Define the forward method\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The replay buffer\n",
    "\n",
    "To implement the replay buffer, we first define a named tuple data type to store transitions. We then define the replay buffer class, which stores transitions and can sample mini-batches of transitions. The replay buffer has the following methods:\n",
    "1. **Push**: This method stores a new transition in the buffer, and removes the oldest transition if the buffer is full\n",
    "2. **Sample**: This method samples a mini-batch of transitions from the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transition named tuple\n",
    "# This will be used to store the transitions (state, action, reward, next_state, terminated) in the replay buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'terminated'))\n",
    "\n",
    "# Define the ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    # Initialise the buffer with a capacity. \n",
    "    def __init__(self, capacity):\n",
    "        # Use a deque object to implement the buffer with the maxlen parameter set to the capacity\n",
    "        \n",
    "\n",
    "    # Define the push method to add a transition to the buffer\n",
    "    def push(self, state, action, reward, next_state, terminated):\n",
    "        # Use the append method to add a transition to the buffer\n",
    "\n",
    "\n",
    "    # Sample a batch of transitions for training \n",
    "    def sample(self, batch_size):\n",
    "        # Use the random.sample method to sample a batch of transitions from the buffer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DQN agent\n",
    "We now define the DQN agent class. This class will have:\n",
    "1. **A value network**, implemented using the Q_net class\n",
    "2. **A target network**, implemented using the Q_net class\n",
    "3. **A replay buffer**, implemented using the ReplayBuffer class\n",
    "4. **An optimiser**, which is used to train the value network\n",
    "\n",
    "The DQN agent will have the following methods:\n",
    "1. **Greedy policy**: This method selects the action with the highest value\n",
    "2. **$\\epsilon$-greedy policy**: This method selects a random action with probability $\\epsilon$, and the action with the highest value with probability 1-$\\epsilon$\n",
    "3. **Sync**: This method synchronises the target network with the value network by loading the parameters of the value network into the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN_agent class\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, buffer_capacity=______, gamma=____, epsilon=____, lr=____, batch_size=__):\n",
    "        # Save the hyperparameters (buffer_capacity, gamma, epsilon, lr, batch_size)\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Save the number of actions and the observation size\n",
    "        self.n_actions = n_actions\n",
    "        self.obs_size = obs_size\n",
    "\n",
    "        # Define the value network for the agent. \n",
    "\n",
    "        # Define the target network for the agent.\n",
    "        \n",
    "        # Sync the networks\n",
    "        \n",
    "        # Define the replay buffer\n",
    "        \n",
    "        # Define the optimiser\n",
    "        \n",
    "\n",
    "\n",
    "    # Define the sync method to sync the target network with the value network\n",
    "    def sync(self):\n",
    "        # Load the state dict of the value network into the target network\n",
    "\n",
    "\n",
    "    # Define the greedy policy\n",
    "    def greedy_policy(self, state):\n",
    "        # Enter no-gradient mode (since we are not training the network when we sample actions)\n",
    "        with torch.no_grad():\n",
    "            # Convert the state to a tensor\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            # Compute the Q values for the state using the value network\n",
    "            \n",
    "            # Find the action with the highest Q value, converting it to a integer\n",
    "            max_action = q_values.argmax().item()\n",
    "            # Return the action\n",
    "            return max_action\n",
    "            \n",
    "\n",
    "    # Define the epsilon greedy policy \n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        # Sample a random number uniformly between 0 and 1 using numpy's random method\n",
    "        rand_num = np.random.random()\n",
    "\n",
    "        # If the random number is less than the exploration rate, choose a random action\n",
    "        if rand_num < self.epsilon:\n",
    "            # Choose a random action using the numpy randint method \n",
    "            \n",
    "            # Return the action\n",
    "            return action\n",
    "        \n",
    "        # Otherwise, choose the action with the highest Q value\n",
    "        else:\n",
    "            # Use the greedy policy to choose the action\n",
    "            \n",
    "            # Return the action\n",
    "            return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the environment\n",
    "During training, the DQN agent must interact with the environment in order to collect transitions to be added to the replay buffer. We define an **interact** function that takes in the environment, the DQN agent, and the number of time steps to interact for. This function will use the $\\epsilon$-greedy policy to select actions, and will store the transitions in the replay buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interact method\n",
    "# This method takes in an agent, an environment, and the number of steps to interact for.\n",
    "def interact(agent, env, n_steps):\n",
    "    # Loop over the steps\n",
    "        \n",
    "        # Get the current state of the environment\n",
    "        \n",
    "        # Choose an action using the epsilon greedy policy\n",
    "        \n",
    "        # Take a step in the environment using the action. \n",
    "        # This returns the next state, reward, terminated, truncated, and info variables\n",
    "        \n",
    "        # Push the transition to the replay buffer\n",
    "\n",
    "        # Reset the environment if the episode is terminated or truncated\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DQN agent\n",
    "\n",
    "We define a **update weights** function that updates the weights of the value network for a DQN agent. This function will:\n",
    "1. Sample a mini-batch of transitions from the replay buffer\n",
    "2. Extracts the states, actions, rewards, next_states, and terminated flags from the transitions\n",
    "3. Uses the target network, rewards, and terminated flags to compute the target values for the value network \n",
    "4. Computes the loss between the value network predictions and the target values\n",
    "5. Runs backpropagation and updates the weights of the value network using that loss\n",
    "\n",
    "As a reminder, the regression targets for $Q(S,A)$ are given by:\n",
    "$$ R + \\gamma (1-\\text{terminated}) \\max_{A'} Q(S',A')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method updates the weights of the value-network of the agent using a batch of transitions\n",
    "def update_weights(agent):\n",
    "    # Sample a batch of transitions from the replay buffer\n",
    "    batch = agent.replay_buffer.sample(agent.batch_size)\n",
    "    \n",
    "    # Extract the batch of states as float32 tensors \n",
    "    states = torch.tensor([transition.state for transition in batch], dtype=torch.float32)\n",
    "    # Extract the batch of actions as int64 tensors\n",
    "    actions = torch.tensor([transition.action for transition in batch], dtype=torch.int64)\n",
    "    # Extract the batch of rewards as float32 tensors\n",
    "    rewards = torch.tensor([transition.reward for transition in batch], dtype=torch.float32)\n",
    "    # Extract the batch of next states as float32 tensors\n",
    "    next_states = torch.tensor([transition.next_state for transition in batch], dtype=torch.float32)\n",
    "    # Extract the batch of terminated flags as bool tensors\n",
    "    terminated = torch.tensor([transition.terminated for transition in batch], dtype=torch.bool)\n",
    "\n",
    "    # Compute the next_state_values using the target network\n",
    "\n",
    "    # Take the max over the actions (dim=1) \n",
    "    max_next_values = next_state_values.max(dim=1)[0] \n",
    "    # Zero out the max-next-values for the terminal states\n",
    "    \n",
    "    # Compute the target values\n",
    "    \n",
    "\n",
    "    # Compute the q_values for all actions using the value network\n",
    "    \n",
    "    # Compute the q_values for the actions that were taken\n",
    "    q_SA = q_values[torch.arange(q_values.size(0)), actions]\n",
    "    # Compute the loss using the mean squared error\n",
    "    \n",
    "\n",
    "    # Zero the gradients using the optimiser\n",
    "    \n",
    "    # Compute the gradients of the loss through backpropagation\n",
    "    \n",
    "    # Update the weights of the value network using the gradients\n",
    "    \n",
    "\n",
    "    # return the loss (as a float)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop for the agent\n",
    "We now put everything together into a training loop for the DQN agent. This loop will:\n",
    "1. Interact with the environment for a number of time steps\n",
    "2. Update the weights of the value network\n",
    "3. Synchronize the target network with the value network at regular intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_loop method\n",
    "def train_loop(agent, env, interactions_per_update, update_steps, sync_delay):\n",
    "    # Initialise the environment\n",
    "    \n",
    "\n",
    "    # Initialise the list of losses\n",
    "    \n",
    "\n",
    "    # Gather initial interactions for the experience replay buffer\n",
    "    \n",
    "\n",
    "    # Loop over the update_steps\n",
    "    \n",
    "        # Interact with the environment\n",
    "\n",
    "\n",
    "        # Update the weights, appending the loss to the list of losses\n",
    "        \n",
    "\n",
    "        # Sync the networks every sync_delay steps\n",
    "        \n",
    "\n",
    "    \n",
    "    # Return the losses\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "We define some helper functions to:\n",
    "1. Evaluate the agent's performance\n",
    "2. Visualise the agent's performance\n",
    "3. Plot the (smoothed) losses during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate function\n",
    "def evaluate(agent, env, n_episodes):\n",
    "    # Initialise the list of rewards\n",
    "    rewards = []\n",
    "    \n",
    "    # Loop over the episodes\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        # Reset the environment\n",
    "        env.reset()\n",
    "        # Get the initial state\n",
    "        state = env.state\n",
    "        # Initialise the episode reward\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Loop over the steps\n",
    "        while True:\n",
    "            # Choose the action with the highest Q value\n",
    "            action = agent.greedy_policy(state)\n",
    "            # Take the action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            # Update the state and reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            # Break if the episode has terminated\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Append the episode reward to the list of rewards\n",
    "        rewards.append(episode_reward)\n",
    "    # Return the mean of the rewards\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the visualise function\n",
    "# This displays the agent's behaviour in the environment for 500 steps.  \n",
    "def visualise(agent, env, n_steps):\n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "\n",
    "    # Initialise the list of frames   \n",
    "    frames = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Render the environment and store the frame\n",
    "        frames.append(env.render())\n",
    "\n",
    "        # Take an action using the greedy policy\n",
    "        state = env.state\n",
    "        action = agent.greedy_policy(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            env.reset()\n",
    "\n",
    "    # Display the movie\n",
    "    for frame in frames:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        sleep(0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a smoothed version of the losses for the regression training\n",
    "def plot_losses(losses):\n",
    "    # Create a smoothed version of the losses\n",
    "    smoothed_losses = np.convolve(losses, np.ones(1000)/1000, mode='valid') \n",
    "    plt.plot(smoothed_losses)\n",
    "    plt.xlabel('Update step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlim(0, len(smoothed_losses))\n",
    "    plt.ylim(0, 1.1*max(smoothed_losses))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's gooooo\n",
    "\n",
    "We will now train our network using the DQN algorithm and visualise the agent's performance. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment, setting the render mode to 'rgb_array'\n",
    "\n",
    "# Reset the environment\n",
    "\n",
    "# Create the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's performance before training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the agent's behaviour before training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent, saving the losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's performance after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the agent's behaviour after training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
