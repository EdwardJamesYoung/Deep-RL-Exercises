{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cartpole environment\n",
    "\n",
    "Cartpole is a classic RL control task. A cart lies on a track and has a pole attached to it which can freely rotate. At each time step, the cart may choose to move left or right. The goal is to keep the pole upright for as long as possible. \n",
    "\n",
    "### Termination and truncations conditions\n",
    "The environment terminates if the pole angle is more than 12 degrees from vertical, or if the cart position is more than 2.4 units from the center.\n",
    "The environment is truncated after 500 time steps, if the pole is still upright.\n",
    "\n",
    "### Rewards\n",
    "The agent recieves a reward of +1 at each time step, as long as the pole is upright.\n",
    "\n",
    "### Observations\n",
    "At each time point, the RL agent recieves the following observations:\n",
    "1. Cart position\n",
    "2. Cart velocity \n",
    "3. Pole angle from vertical\n",
    "4. Pole anglular velocity \n",
    "\n",
    "### Action space\n",
    "The agent can take one of two actions at each time step:\n",
    "1. Move the cart to the left\n",
    "2. Move the cart to the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN algorithm components\n",
    "\n",
    "The DQN algorithm requires the following components, with the following roles:\n",
    "1. **Value network**: A neural network that takes in states and outputs predictions for the value of each action in that state\n",
    "2. **Target network**: A copy of the value network with lagged parameters that is used to compute the target values for the regression targets\n",
    "3. **Replay buffer**: A buffer that stores transitions (state, action, reward, next_state, terminated) that the agent has experienced. This is used to sample mini-batches of transitions to train the value network\n",
    "4. **Policies**: These use the value network to select actions in the environment. We will consider two policies:\n",
    "    - **Greedy policy**: This policy selects the action with the highest value, and is used to evaluate the value network\n",
    "    - **$\\epsilon$-greedy policy**: This policy selects a random action with probability $\\epsilon$, and the action with the highest value with probability 1-$\\epsilon$. This is used to explore the environment using the training phase. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The value and target networks\n",
    "\n",
    "We start by defining the value network architecture. This is a simple feedforward neural network (a **multi-layer perceptron**). \n",
    "The input to the network is the state of the environment, and the output is the value prediction of each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q_net class\n",
    "\n",
    "    # Initialise the network using the size of the observation space and the number of actions\n",
    "    \n",
    "        # Use the nn.Module's __init__ method to ensure that the parameters can be updated during training\n",
    "        \n",
    "\n",
    "        # Define the layers of the network\n",
    "        \n",
    "\n",
    "\n",
    "    # Define the forward method\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The replay buffer\n",
    "\n",
    "To implement the replay buffer, we first define a named tuple data type to store transitions. We then define the replay buffer class, which stores transitions and can sample mini-batches of transitions. The replay buffer has the following methods:\n",
    "1. **Push**: This method stores a new transition in the buffer, and removes the oldest transition if the buffer is full\n",
    "2. **Sample**: This method samples a mini-batch of transitions from the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transition named tuple\n",
    "# This will be used to store the transitions (state, action, reward, next_state, terminated) in the replay buffer\n",
    "\n",
    "\n",
    "# Define the ReplayBuffer class\n",
    "\n",
    "    # Initialise the buffer with a capacity argument. \n",
    "\n",
    "        # Use a deque object to implement the buffer\n",
    "\n",
    "\n",
    "    # Define the push method to add a transition to the buffer\n",
    "    \n",
    "\n",
    "    # Define a sample method to randomly sample a batch of transitions for training\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DQN agent\n",
    "We now define the DQN agent class. This class will have:\n",
    "1. **A value network**, implemented using the Q_net class\n",
    "2. **A target network**, implemented using the Q_net class\n",
    "3. **A replay buffer**, implemented using the ReplayBuffer class\n",
    "4. **An optimiser**, which is used to train the value network\n",
    "\n",
    "The DQN agent will have the following methods:\n",
    "1. **Greedy policy**: This method selects the action with the highest value\n",
    "2. **$\\epsilon$-greedy policy**: This method selects a random action with probability $\\epsilon$, and the action with the highest value with probability 1-$\\epsilon$\n",
    "3. **Sync**: This method synchronises the target network with the value network by loading the parameters of the value network into the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN_agent class\n",
    "\n",
    "    # Create the initialisation method for the agent\n",
    "\n",
    "        # Save the hyperparameters (buffer_capacity, gamma, epsilon, lr, batch_size)\n",
    "\n",
    "\n",
    "        # Save the number of actions and the observation size\n",
    "\n",
    "\n",
    "        # Define the value network for the agent. \n",
    "\n",
    "        # Define the target network for the agent.\n",
    "\n",
    "        # Sync the networks\n",
    "\n",
    "        # Define the replay buffer\n",
    "\n",
    "        # Define the optimizer\n",
    "\n",
    "\n",
    "\n",
    "    # Define the sync method to sync the target network with the value network\n",
    "\n",
    "\n",
    "\n",
    "    # Define the greedy policy\n",
    "\n",
    "        # Enter no-gradient mode (since we are not training the network when we sample actions)\n",
    "\n",
    "            # Convert the state to a tensor\n",
    "\n",
    "            # Compute the Q values for the state using the value network\n",
    "\n",
    "            # Find the action with the highest Q value, converting it to a integer\n",
    "\n",
    "            # Return the action\n",
    "\n",
    "\n",
    "\n",
    "    # Define the epsilon greedy policy \n",
    "\n",
    "        # Sample a random number uniformly betwen 0 and 1  \n",
    "\n",
    "        # If the random number is less than the exploration rate, choose a random action\n",
    "\n",
    "            # Choose a random action\n",
    "\n",
    "            # Return the action\n",
    "\n",
    "\n",
    "        # Otherwise, choose the action with the highest Q value\n",
    "\n",
    "            # Use the greedy policy to choose the action\n",
    "\n",
    "            # Return the action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the environment\n",
    "During training, the DQN agent must interact with the environment in order to collect transitions to be added to the replay buffer. We define an **interact** function that takes in the environment, the DQN agent, and the number of time steps to interact for. This function will use the $\\epsilon$-greedy policy to select actions, and will store the transitions in the replay buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interact method\n",
    "# This method takes in an agent, an environment, and the number of steps to interact for.\n",
    "\n",
    "    # Loop over the steps\n",
    "\n",
    "        # Get the current state\n",
    "\n",
    "        # Choose an action using the epsilon greedy policy\n",
    "\n",
    "        # Take a step in the environment using the action\n",
    "\n",
    "        # Push the transition to the replay buffer\n",
    "\n",
    "        # Reset the environment if the episode is terminated or truncated\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DQN agent\n",
    "\n",
    "We define a **update weights** function that updates the weights of the value network for a DQN agent. This function will:\n",
    "1. Sample a mini-batch of transitions from the replay buffer\n",
    "2. Extracts the states, actions, rewards, next_states, and terminated flags from the transitions\n",
    "3. Uses the target network, rewards, and terminated flags to compute the target values for the value network \n",
    "4. Computes the loss between the value network predictions and the target values\n",
    "5. Runs backpropagation and updates the weights of the value network using that loss\n",
    "\n",
    "As a reminder, the regression targets for $Q(S,A)$ are given by:\n",
    "$$ R + \\gamma (1-\\text{terminated}) \\max_{A'} Q(S',A')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method updates the weights of the value-network of the agent using a batch of transitions\n",
    "\n",
    "    # Sample a batch of transitions from the replay buffer\n",
    "\n",
    "\n",
    "    # Extract the batch of states as float32 tensors \n",
    "\n",
    "    # Extract the batch of actions as int64 tensors\n",
    "\n",
    "    # Extract the batch of rewards as float32 tensors\n",
    "\n",
    "    # Extract the batch of next states as float32 tensors\n",
    "\n",
    "    # Extract the batch of terminated flags as bool tensors\n",
    "\n",
    "\n",
    "    # Compute the next state values using the target network\n",
    "\n",
    "    # Take the max over the actions (dim=1) \n",
    "\n",
    "    # Zero out the max-next-values for the terminal states\n",
    "\n",
    "    # Compute the target values\n",
    "\n",
    "\n",
    "    # Compute the Q values for all actions\n",
    "\n",
    "    # Compute the Q values for the actions that were taken\n",
    "\n",
    "    # Compute the loss using the mean squared error\n",
    "\n",
    "\n",
    "    # Zero the gradients using the optimiser\n",
    "\n",
    "    # Compute the gradients of the loss through backpropagation\n",
    "\n",
    "    # Update the weights of the value network using the gradients\n",
    "\n",
    "\n",
    "    # return the loss (as a float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop for the agent\n",
    "We now put everything together into a training loop for the DQN agent. This loop will:\n",
    "1. Interact with the environment for a number of time steps\n",
    "2. Update the weights of the value network\n",
    "3. Synchronize the target network with the value network at regular intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_loop method\n",
    "\n",
    "    # Initialise the environment\n",
    "\n",
    "\n",
    "    # Initialise the list of losses\n",
    "\n",
    "\n",
    "    # Gather initial interactions for the experience replay buffer\n",
    "\n",
    "\n",
    "    # Loop over the update_steps\n",
    "\n",
    "        # Interact with the environment\n",
    "\n",
    "        \n",
    "        # Update the weights\n",
    "\n",
    "\n",
    "        # Sync the networks every sync_delay steps\n",
    "\n",
    "\n",
    "    # Return the losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "We define some helper functions to:\n",
    "1. Evaluate the agent's performance\n",
    "2. Visualise the agent's performance\n",
    "3. Plot the (smoothed) losses during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate function\n",
    "\n",
    "    # Initialise the list of rewards\n",
    "\n",
    "\n",
    "    # Loop over the episodes\n",
    "\n",
    "        # Reset the environment\n",
    "\n",
    "        # Get the initial state\n",
    "\n",
    "        # Initialise the episode reward\n",
    "\n",
    "\n",
    "        # Loop over the steps\n",
    "\n",
    "            # Choose the action with the highest Q value\n",
    "\n",
    "            # Take the action\n",
    "\n",
    "            # Update the state and reward\n",
    "\n",
    "\n",
    "            # Break if the episode has terminated\n",
    "\n",
    "\n",
    "        # Append the episode reward to the list of rewards\n",
    "\n",
    "    # Return the mean of the rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the visualise function\n",
    "# This displays the agent's behaviour in the environment for 500 steps.  \n",
    "\n",
    "    # Reset the environment\n",
    "\n",
    "\n",
    "    # Initialise the list of frames   \n",
    "\n",
    "    # Loop over the steps\n",
    "\n",
    "        # Render the environment and store the frame\n",
    "\n",
    "        # Set state to the current state of the environment\n",
    "        \n",
    "        # Choose the action with the greedy policy\n",
    "\n",
    "        # Take the action in the environment\n",
    "\n",
    "        # Reset the environment if the episode is terminated or truncated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Display the movie by looping over the frames\n",
    "    \n",
    "        # Clear the previous frame\n",
    "        \n",
    "        # Use imshow to display the frame\n",
    "        \n",
    "        # Show the frame\n",
    "        \n",
    "        # Pause for a short time (0.003 seconds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a smoothed version of the losses for the regression training\n",
    "\n",
    "    # Create a smoothed version of the losses\n",
    "    \n",
    "    # Plot the smoothed losses\n",
    "    \n",
    "    # Set the x-label to 'Update step'\n",
    "    \n",
    "    # Set the y-label to 'Loss'\n",
    "    \n",
    "    # Set the x-axis limits to 0 and the length of the smoothed losses\n",
    "    \n",
    "    # Set the y-axis limits to 0 and 1.1 times the maximum smoothed loss\n",
    "    \n",
    "    # Show the plot\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's gooooo\n",
    "\n",
    "We will now train our network using the DQN algorithm and visualise the agent's performance. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cartpole environment with the render mode set to 'rgb_array'\n",
    "\n",
    "# Reset the environment\n",
    "\n",
    "# Create the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's performance before training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the agent's behaviour before training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent, saving the losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's performance after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the agent's behaviour after training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
